{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "otu_table = 'Input/merged_otu.tab'\n",
    "map_file = 'Input/merged_map.tab'\n",
    "sample_ids = 'Input/sample_ids.txt'\n",
    "timepoint_1 = 'Input/wk1.txt'\n",
    "status = 'Phenotype'\n",
    "wk_5 = \"Input/5wk.txt\"\n",
    "wk_12 = \"Input/wk12cc_tgtg.csv\"\n",
    "# set input variables\n",
    "\n",
    "otu = pd.read_table(otu_table , sep='\\t', index_col=0)\n",
    "metadata = pd.read_table(map_file ,sep='\\t', index_col=0) \n",
    "ids = pd.read_table(sample_ids, sep='\\t')\n",
    "faecal_1 = pd.read_table(timepoint_1, sep='\\t')\n",
    "w5 = pd.read_table(wk_5, sep='\\t')\n",
    "w12 = pd.read_csv(wk_12, sep=\",\")\n",
    "# load data and metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sum_row = otu.sum()\n",
    "min_sum = min(otu.sum())\n",
    "normalise = otu * 100 / sum_row \n",
    "\n",
    "# normalise data: relative abundance - multiply each row by 100 and divide by the sum of the row\n",
    "normalise.to_csv(\"otu_for_heatmap.csv\", sep=\",\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop associated samples \n",
    "\n",
    "id_list = ids['#SampleID'].tolist()\n",
    "train_wk5 = w5['#SampleID'].tolist()\n",
    "association = faecal_1['#SampleID'].tolist()\n",
    "wk12_list = w12['#SampleID'].tolist() \n",
    "\n",
    "training = [x for x in id_list if x in train_wk5]\n",
    "# filter id_list so dataset only includes wk5 samples\n",
    "\n",
    "\n",
    "test_train = normalise[training]\n",
    "\n",
    "# create filtered df to train on \n",
    "test_train.to_csv(\"5wk_otu.csv\", sep=',',encoding='utf-8')\n",
    "metadata_5 = metadata_trans[train_wk5]\n",
    "metadata_5 = metadata_5.transpose()\n",
    "metadata_5.to_csv(\"metadata_5wk.csv\", sep=\",\", encoding='utf-8')\n",
    "\n",
    "metadata_trans = metadata.transpose()\n",
    "metadata_asc = metadata_trans[training]\n",
    "metadata_asc = metadata_asc.transpose()\n",
    "metadata_asc.to_csv(\"metadata_asc.csv\", sep=',',encoding='utf-8')\n",
    "\n",
    "wk1 = [x for x in id_list if x in association]\n",
    "wk1_predict = normalise[wk1]\n",
    "\n",
    "wk1_predict.to_csv(\"association_to_predict.csv\", sep=',', encoding='utf-8')\n",
    "\n",
    "# 12wk samples \n",
    "metadata_12 = metadata_trans[wk12_list]\n",
    "metadata_12 = metadata_12.transpose()\n",
    "\n",
    "otu_12 = normalise[wk12_list]\n",
    "metadata_12.to_csv(\"metadata_12_cc.csv\", sep=',', encoding='utf-8')\n",
    "otu_12.to_csv(\"otu_12_cc.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
